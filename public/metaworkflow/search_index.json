[
["index.html", "Meta-Workflow Preface", " Meta-Workflow Miao YU 2017-01-30 Preface This is a book written in Bookdown. You could contribute it by a pull request in Github. R and Rstudio are the softwares needed in this workflow. The software package used for metabolomics data analysis is xcms. "],
["introduction.html", "Chapter 1 Introduction 1.1 Reviews and tutorials", " Chapter 1 Introduction Information in living organism commuicates along the Genomics, Transcriptomics, Proteomics and Metabolomics in Central dogma. Following such stream, we might answer certain problems in different scales from individual, population, community to ecosystem. Metabolomics (i.e., the profiling and quantitation of metabolites in body fluids) is a relatively new field of “omics” studies. Different from other omics studies, metabolomics always focused on small moleculars with much lower mass than polypeptide, around m/z 100-1000. Metabolomics studies are always performed in GC/MS, LC/MS or NMR. This workflow concerns mass spectrum based metabolomics. 1.1 Reviews and tutorials Some new reviews and tutorials related to this workflow could be found in those papers(Alonso, Marsal, and Julià 2015; Cajka and Fiehn 2016; Lu and Xu 2008; Schrimpe-Rutledge et al. 2016; Townsend et al. 2016; Barnes et al. 2016b; Barnes et al. 2016a). References "],
["exprimental-designdoe.html", "Chapter 2 Exprimental design(DoE)", " Chapter 2 Exprimental design(DoE) Before you perform any metabolomic studies, a clean and meaningful experimental design is the best start. You need at least two groups: treated group and control group. Also you could treat this group infomation as the one primary variable or primary variables to be explored for certain research purposes. The numbers of samples in each group should be carefully calculated. Supposing the metaoblites of certain biological process only have a few metabolites, the first goal of the exprimenal design is to find the differences of each metabolite in different group. For each metabolite, such comparision could be treated as one t-test. You need to perform a Power analysis to get the numbers. For example, we have two groups of samples with 10 samples in each group. Then we set the power at 0.9, which means 1 minus Type II error probability, the standard deviation at 1 and the significance level(Type 1 error probability) at 0.05. Then we get the meanful delta between the two groups should be higher than 1.53367 under this experiment design. Also we could set the delta to get the minimized numbers of the samples in each group. To get those data such as the standard deviation or delta for power analysis, you need to perform pre-experiments. power.t.test(n=10,sd=1,sig.level = 0.05,power = 0.9) ## ## Two-sample t test power calculation ## ## n = 10 ## delta = 1.53367 ## sd = 1 ## sig.level = 0.05 ## power = 0.9 ## alternative = two.sided ## ## NOTE: n is number in *each* group power.t.test(delta = 5,sd=1,sig.level = 0.05,power = 0.9) ## ## Two-sample t test power calculation ## ## n = 2.328877 ## delta = 5 ## sd = 1 ## sig.level = 0.05 ## power = 0.9 ## alternative = two.sided ## ## NOTE: n is number in *each* group If there are other co-factors, a linear model or randomizing would be applied to eliminated their influences. You need to record the values of those co-factors for further data analysis. Common co-factors in metabolomic studies are age, gender, location, etc. If you need data correction, some background or calibration samples are required. However, control samples could also be used for data correction in certain DoE. Another important factors are instrumentals. High-resolution mass spectrum is always preferred. As shown in Lukas’s study Najdekr et al. (2016): the most effective mass resolving powers for profiling analyses of metabolite rich biofluids on the Orbitrap Elite were around 60000–120000 fwhm to retrieve the highest amount of information. The region between 400–800 m/z was influenced the most by resolution. References "],
["pretreatment.html", "Chapter 3 Pretreatment", " Chapter 3 Pretreatment Pretreatment will affect the results of metabolomics. For example, feces collected with 95% ethanol or FOBT would be more reproducible and stable. Dmitri et.al(Sitnikov, Monnin, and Vuckovic 2016) thought the most orthogonal methods to methanol-based precipitation were ion-exchange solid-phase extraction and liquid-liquid extraction using methyl-tertbutyl ether. References "],
["raw-data-pretreatment.html", "Chapter 4 Raw data pretreatment 4.1 Spectral deconvolution 4.2 Correction 4.3 Dynamic Range 4.4 Adjust for unwanted variances with known batch 4.5 Adjust for unwanted variance with unknown batch", " Chapter 4 Raw data pretreatment Raw data from the instruments such as LC-MS or GC-MS were hard to be analyzed. To make it clear, the structure of those data could be summarised as: to get full infomation in the samples, full scan is perferred full scan is performed synchronously with the seperation process GC/LC-MS data are usually be shown as a matrix with column standing for retention times and row standing for masses. Noises are so much that such data could not be processed effeciently. Figure 4.1: Demo of GC/LC-MS data Conversation from the mass-retention time matrix into a vector with selected MS peaks at certain retention time is the basic idea of the Raw data pretreatment. The Centwave algorithm(Tautenhahn, Böttcher, and Neumann 2008) based on detection of regions of interest(ROI) and the following Continuous Wavelet Transform (CWT) for the peaks is preferred for high-resolution mass spectrum. With many groups of samples, you will get another data matrix with column standing for ions at cerntain retention time and row standing for samples after the Raw data pretreatment. Figure 4.2: Demo of many GC/LC-MS data 4.1 Spectral deconvolution Without fracmental infomation about certain compound, the peak extraction would suffer influnces from other compounds. At the same retention time, co-elute compounds might share similar mass. Hard electron ionization methods such as electron impact ionization (EI), APPI suffer this issue. So it would be hard to distighuish the co-elute peaks’ origin and deconvolution method(Du and Zeisel 2013) could be used to seperate different groups according to the similar chromatogragh beheviors. Another computational tool eRah could be a better solution for the whole process(Domingo-Almenara et al. 2016). Also the ADAD-GC3.0 could also be helpful for such issue(Ni et al. 2016). 4.2 Correction However, before you get the peaks, some corrections should be performed such as mass shift and retention time shift. The basic idea behind retention time correction is that use the high quality grouped peaks to make a new retention time. You might choose obiwarp or loess regression method to get the corrected retention time for all of the samples. Remember the original retention times might be changed and you might need cross-correct the data. 4.3 Dynamic Range Another issue is the Dynamic Range. For metabolomics, peaks could be below the detection limit or over the detection limit. Such Dynamic range issues might raise the loss of information. 4.3.1 Non-detects Some of the data were limited by the detect of limitation. Thus we need some methods to impute the data if we don’t want to lose information by deleting the NA or 0. Tobit regression is preferred. Also you might choose maximum likelihood estimation(Estimation of mean and standard deviation by MLE. Creating 10 complete samples. Pool the results from 10 individual analyses). x &lt;- rnorm(1000,1) x[x&lt;0] &lt;- 0 y &lt;- x*10+1 library(AER) ## Loading required package: car ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich ## Loading required package: survival tfit &lt;- tobit(y ~ x, left = 0) summary(tfit) ## ## Call: ## tobit(formula = y ~ x, left = 0) ## ## Observations: ## Total Left-censored Uncensored Right-censored ## 1000 0 1000 0 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.0000 0.4405 2.27 0.0232 * ## x 10.0000 0.3162 31.62 &lt;2e-16 *** ## Log(scale) 2.1744 0.0000 Inf &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Scale: 8.797 ## ## Gaussian distribution ## Number of Newton-Raphson Iterations: 1 ## Log-likelihood: -3093 on 3 Df ## Wald-statistic: 1000 on 1 Df, p-value: &lt; 2.22e-16 4.3.2 Over detection limit CorrectOverloadedPeaks could be used to correct the Peaks Exceeding the Detection Limit issue(Lisec et al. 2016). 4.4 Adjust for unwanted variances with known batch 4.4.1 Centering For peak p of sample s in batch b, the corrected abundance I is: \\[\\hat I_{p,s,b} = I_{p,s,b} - mean(I_{p,b}) + median(I_{p,qc})\\] For example, we have the intensities of one peak from ten samples in two batches like the following demo: set.seed(42) # raw data I = c(rnorm(10,mean = 0, sd = 0.5),rnorm(10,mean = 1, sd = 0.5)) # batch B = c(rep(0,10),rep(1,10)) # qc Iqc = c(rnorm(1,mean = 0, sd = 0.5),rnorm(1,mean = 1, sd = 0.5)) # corrected data Icor = I - c(rep(mean(I[1:10]),10),rep(mean(I[11:20]),10)) + median(Iqc) # plot the result plot(I) plot(Icor) 4.4.2 Scaling For peak p of sample s in certain batch b, the corrected abundance I is: \\[\\hat I_{p,s,b} = \\frac{I_{p,s,b} - mean(I_{p,b})}{std_{p,b}} * std_{p,qc,b} + mean(I_{p,qc,b})\\] For example, we have the intensities of one peak from ten samples in two batches like the following demo: set.seed(42) # raw data I = c(rnorm(10,mean = 0, sd = 0.3),rnorm(10,mean = 1, sd = 0.5)) # batch B = c(rep(0,10),rep(1,10)) # qc Iqc = c(rnorm(1,mean = 0, sd = 0.3),rnorm(1,mean = 1, sd = 0.5)) # corrected data Icor = (I - c(rep(mean(I[1:10]),10),rep(mean(I[11:20]),10)))/c(sd(I[1:10]),sd(I[11:20]))*c(rep(0.3,10),rep(0.5,10)) + Iqc[1] # plot the result plot(I) plot(Icor) 4.4.3 Quantile The idea of quantile calibration is that alignment of the intensities in certain samples according to quantiles in each sample. Here is the demo: set.seed(42) a &lt;- rnorm(1000) # b sufferred batch effect with a bias of 10 b &lt;- rnorm(1000,10) hist(a,xlim=c(-5,15),breaks = 50) hist(b,col = &#39;black&#39;, breaks = 50, add=T) # quantile normalized cor &lt;- (a[order(a)]+b[order(b)])/2 # reorder cor &lt;- cor[order(order(a))] hist(cor,col = &#39;red&#39;, breaks = 50, add=T) 4.4.4 Ratio based calibraton This method calibrates samples by the ratio between qc samples in all samples and in certain batch.For peak p of sample s in certain batch b, the corrected abundance I is: \\[\\hat I_{p,s,b} = \\frac{I_{p,s,b} * median(I_{p,qc})}{mean_{p,qc,b}}\\] set.seed(42) # raw data I = c(rnorm(10,mean = 0, sd = 0.3),rnorm(10,mean = 1, sd = 0.5)) # batch B = c(rep(0,10),rep(1,10)) # qc Iqc = c(rnorm(1,mean = 0, sd = 0.3),rnorm(1,mean = 1, sd = 0.5)) # corrected data Icor = I * median(c(rep(Iqc[1],10),rep(Iqc[2],10)))/mean(c(rep(Iqc[1],10),rep(Iqc[2],10))) # plot the result plot(I) plot(Icor) 4.4.5 Linear Normalizer This method initially scales each sample so that the sum of all peak abundances equals one. In this study, by multiplying the median sum of all peak abundances across all samples,we got the corrected data. set.seed(42) # raw data peaksa &lt;- c(rnorm(10,mean = 10, sd = 0.3),rnorm(10,mean = 20, sd = 0.5)) peaksb &lt;- c(rnorm(10,mean = 10, sd = 0.3),rnorm(10,mean = 20, sd = 0.5)) df &lt;- rbind(peaksa,peaksb) dfcor &lt;- df/apply(df,2,sum)* sum(apply(df,2,median)) image(df) image(dfcor) 4.4.6 Regression calibration Considering the batch effect of injection order, regress the data by a linear model to get the calibration. 4.4.7 Batch Normalizer Use the total abundance scale and then fit with the regression line(Wang, Kuo, and Tseng 2013). 4.4.8 Internal standards \\[\\hat I_{p,s} = \\frac{I_{p,s} * median(I_{IS})}{I_{IS,s}}\\] Some methods also use pooled calibration samples and multiple internal standard strategy to correct the data(???). Also some methods only use QC samples to handle the data(Kuligowski et al. 2015). 4.5 Adjust for unwanted variance with unknown batch 4.5.1 SVA 4.5.2 RUV References "],
["peaks-selection.html", "Chapter 5 Peaks selection", " Chapter 5 Peaks selection After we get corrected peaks across samples, the next step is finding the differences between two groups. Actually, you could perform ANOVA or Kruskal-Wallis Test for comparison among more than two groups. The basic idea behind statistic analysis is to find the meaningful differences between groups and extract such ions or peak groups. So how to find the differences? In most metabolomics software, such task is completed by a t-test and report p-value and fold changes. If you only compare two groups on one peaks, that’s OK. However, if you compare two groups on thousands of peaks, statistic textbook would tell you to notice the false positive. For one comparasion, the confidence level is 0.05, which means 5% chances to get false positive result. For two comparasions, such chances would be \\(1-0.95^2\\). For 10 comparasions, such chances would be \\(1-0.95^{10} = 0.4012631\\). For 100 comparasions, such chances would be \\(1-0.95^{100} = 0.9940795\\). You would almost certainly to make mistakes for your results. In statistics, the false discovery rate(FDR) control is always mentioned in omics studies for mutiple tests. I suggested using q-values to control FDR. If q-value is less than 0.05, we should expect a lower than 5% chances we make the wrong selections for all of the comparisions showed lower q-values in the whole dataset. Also we could use local false discovery rate, which showed the FDR for certain peaks. However, such values are hard to be estimated accurately. "],
["annotation.html", "Chapter 6 Annotation 6.1 Tools", " Chapter 6 Annotation When you get the peaks table or features table, annotation of the peaks would help you. Obviously, peaks with similar retention time might come from the same compound such as isotopic peaks, addictions from the analysis process. CAMERA (Kuhl et al. 2012) or Ramcluster(Broeckling et al. 2014) package could be used to make such annotation. Also this package could be used to group the peaks together for further analysis. There are two major annotation ideas: multi-stage MS analysis and similarity analysis. The former requires certain instruments and database for MS/MS data. The later needs algorithms to predict the structures. 6.1 Tools Plantmat: excel library based pridiction for plant metabolites(Qiu et al. 2016). References "],
["omics-analysis.html", "Chapter 7 Omics analysis 7.1 Pathway analysis 7.2 Network analysis 7.3 Omics integration", " Chapter 7 Omics analysis When you get the filtered ions, the next step is making annotations for them. Such annotations would be helpful for omics studies. Since we have got the annotations, Omics analysis could be performed.Upload the data obtained from the xcms to other tools or databases. You will get an updated database list here Right now, it is hard to connect different omics databases such as gene, protein and metabolites together for a whole scope of certain biological process. However, you might select few metabolites across those databases and find something interesting. 7.1 Pathway analysis 7.2 Network analysis 7.3 Omics integration "],
["common-analysis-methods-for-metabolomics.html", "Chapter 8 Common analysis methods for metabolomics 8.1 PCA 8.2 Cluster Analysis 8.3 PLSDA", " Chapter 8 Common analysis methods for metabolomics 8.1 PCA In most cases, PCA is used as an exploratory data analysis(EDA) method. In most of those most cases, PCA is just served as visualization method. I mean, when I need to visualize some high-dimension data, I would use PCA. So, the basic idea behind PCA is compression. When you have 100 samples with concentrations of certain compound, you could plot the concentrations with samples’ ID. However, if you have 100 compounds to be analyzed, it would by hard to show the relationship between the samples. Actually, you need to show a matrix with sample and compounds (100 * 100 with the concentrations filled into the matrix) in an informal way. The PCA would say: OK, guys, I could convert your data into only 100 * 2 matrix with the loss of information minimized. Yeah, that is what the mathematical guys or computer programmer do. You just run the command of PCA. The new two “compounds” might have the cor-relationship between the original 100 compounds and retain the variances between them. After such projection, you would see the compressed relationship between the 100 samples. If some samples’ data are similar, they would be projected together in new two “compounds” plot. That is why PCA could be used for cluster and the new “compounds” could be referred as principal components(PCs). However, you might ask why only two new compounds could finished such task. I have to say, two PCs are just good for visualization. In most cases, we need to collect PCs standing for more than 80% variances in our data if you want to recovery the data with PCs. If each compound have no relationship between each other, the PCs are still those 100 compounds. So you have found a property of the PCs: PCs are orthogonal between each other. Another issue is how to find the relationship between the compounds. We could use PCA to find the relationship between samples. However, we could also extract the influences of the compounds on certain PCs. You might find many compounds showed the same loading on the first PC. That means the concentrations pattern between the compounds are looked similar. So PCA could also be used to explore the relationship between the compounds. OK, next time you might recall PCA when you need it instead of other paper showed them. 8.2 Cluster Analysis After we got a lot of samples and analyzed the concentrations of many compounds in them, we may ask about the relationship between the samples. You might have the sampling information such as the date and the position and you could use boxplot or violin plot to explore the relationships among those categorical variables. However, you could also use the data to find some potential relationship. But how? if two samples’ data were almost the same, we might think those samples were from the same potential group. On the other hand, how do we define the “same” in the data? Cluster analysis told us that just define a “distances” to measure the similarity between samples. Mathematically, such distances would be shown in many different manners such as the sum of the absolute values of the differences between samples. For example, we analyzed the amounts of compound A, B and C in two samples and get the results: Compounds(ng) A B C Sample 1 10 13 21 Sample 2 54 23 16 The distance could be: \\[ distance = |10-54|+|13-23|+|21-16| = 59 \\] Also you could use the sum of squares or other way to stand for the similarity. After you defined a “distance”, you could get the distances between all of pairs for your samples. If two samples’ distance was the smallest, put them together as one group. Then calculate the distances again to combine the small group into big group until all of the samples were include in one group. Then draw a dendrogram for those process. The following issue is that how to cluster samples? You might set a cut-off and directly get the group from the dendrogram. However, sometimes you were ordered to cluster the samples into certain numbers of groups such as three. In such situation, you need K means cluster analysis. The basic idea behind the K means is that generate three virtual samples and calculate the distances between those three virtual samples and all of the other samples. There would be three values for each samples. Choose the smallest values and class that sample into this group. Then your samples were classified into three groups. You need to calculate the center of those three groups and get three new virtual samples. Repeat such process until the group members unchanged and you get your samples classified. OK, the basic idea behind the cluster analysis could be summarized as define the distances, set your cut-off and find the group. By this way, you might show potential relationships among samples. 8.3 PLSDA "],
["demo.html", "Chapter 9 Demo 9.1 Project Setup 9.2 Data input 9.3 Find the peaks 9.4 Data correction 9.5 Statistic analysis 9.6 Annotation 9.7 Omics analysis 9.8 MetaboAnalyst 9.9 Visulizing Peaks 9.10 Optimation of XCMS 9.11 Summary", " Chapter 9 Demo 9.1 Project Setup I suggest building your data analysis projects in RStudio(Click File - New project - New dictionary - Empty project). Then assign a name for your project. I also recommend the following tips if you are familiar with it. Use git/github to make version control of your code and sync your project online. NOT use your name for your project because other peoples might cooperate with you and someone might check your data when you publish your papers. Each project should be a work for one paper or one chapter in your thesis. Use workflow document(txt or doc) in your project to record all of the steps and code you performed for this project. Treat this document as digital version of your experiment notebook Use data folder in your project folder for the raw data and the results you get in data analysis Use figure folder in your project folder for the figure Use munuscript folder in your project folder for the manuscript (you could write paper in rstudio with the help of template in Rmarkdown) Just double click [yourprojectname].Rproj to start your project 9.2 Data input xcms does not support all of the Raw files from every mass spectrometry manufacturers. You need to convert your Raw data into some open-source data format such as mzData, mzXML or CDF files. The tool is MScovert from ProteoWizard. Here is a demo: # install the packages for data analysis and # source(&quot;https://bioconductor.org/biocLite.R&quot;) # biocLite(c(&quot;multtest&quot;,&quot;faahKO&quot;,&quot;xcms&quot;,&quot;qvalue&quot;,&quot;CAMERA&quot;)) # load the functions and dataset for demo library(multtest) library(xcms) library(faahKO) library(BiocParallel) # get the demo data in faahKO packages cdfpath &lt;- system.file(&quot;cdf&quot;,package = &quot;faahKO&quot;) # show the name of demo data list.files(cdfpath,recursive = T) ## [1] &quot;KO/ko15.CDF&quot; &quot;KO/ko16.CDF&quot; &quot;KO/ko18.CDF&quot; &quot;KO/ko19.CDF&quot; &quot;KO/ko21.CDF&quot; ## [6] &quot;KO/ko22.CDF&quot; &quot;WT/wt15.CDF&quot; &quot;WT/wt16.CDF&quot; &quot;WT/wt18.CDF&quot; &quot;WT/wt19.CDF&quot; ## [11] &quot;WT/wt21.CDF&quot; &quot;WT/wt22.CDF&quot; Here is a demo for xcmsSet: cdffiles &lt;- list.files(cdfpath, recursive = TRUE, full.names = TRUE) xset &lt;- xcmsSet(cdffiles,BPPARAM=SnowParam()) ## 250:38 300:103 350:226 400:338 450:431 500:529 550:674 600:847 ## 250:43 300:128 350:275 400:394 450:500 500:637 550:835 600:1027 ## 250:25 300:93 350:227 400:337 450:411 500:498 550:640 600:758 ## 250:19 300:67 350:169 400:258 450:301 500:373 550:488 600:580 ## 250:24 300:60 350:166 400:254 450:315 500:391 550:501 600:582 ## 250:31 300:71 350:183 400:280 450:338 500:422 550:532 600:604 ## 250:41 300:105 350:212 400:319 450:416 500:533 550:684 600:838 ## 250:27 300:107 350:232 400:347 450:440 500:549 550:712 600:905 ## 250:24 300:87 350:200 400:293 450:351 500:426 550:548 600:661 ## 250:22 300:65 350:161 400:243 450:293 500:358 550:483 600:561 ## 250:28 300:69 350:157 400:229 450:282 500:364 550:493 600:592 ## 250:30 300:81 350:188 400:280 450:356 500:473 550:618 600:765 xset ## An &quot;xcmsSet&quot; object with 12 samples ## ## Time range: 2506.1-4147.7 seconds (41.8-69.1 minutes) ## Mass range: 200.1-599.3338 m/z ## Peaks: 4721 (about 393 per sample) ## Peak Groups: 0 ## Sample classes: KO, WT ## ## Peak picking was performed on MS1. ## Profile settings: method = bin ## step = 0.1 ## ## Memory usage: 0.713 MB 9.3 Find the peaks The first step to process the MS data is that find the peaks against the noises. In xcms, all of related staffs are handled by xcmsSet function. For any functions in xcms or R, you could get their documents by type ? before certain function. Another geek way is input the name of the function in the console of Rstudio and press F1 for help. ?xcmsSet In the document of xcmsset, we could set the sample classes, profmethod, profparam, polarity,etc. In the online version, such configurations are shown in certain windows. In the local analysis environment, such parameters are setup by yourselves. However, I think the default configurations could satisfied most of the analysis because related information should have been recorded in your Raw data and xcms could find them. All you need to do is that show the data dictionary for xcmsSet. If your data have many groups such as control and treated group, just put them in separate subfolder of the data folder and xcmsSet would read them as separated groups. The output was an object with class of xcmsSet. You could see a summary by type the name. In this cases, xcmsSet found 4721 peaks with time range 41.8-69.1 min and mass range 200.1-599.3338 m/z in the 12 samples. Another function which might be useful is group. This function will add additional information about the same analytes for xcmsSet objects. xset &lt;- group(xset) ## 262 325 387 450 512 575 xset ## An &quot;xcmsSet&quot; object with 12 samples ## ## Time range: 2506.1-4147.7 seconds (41.8-69.1 minutes) ## Mass range: 200.1-599.3338 m/z ## Peaks: 4721 (about 393 per sample) ## Peak Groups: 403 ## Sample classes: KO, WT ## ## Peak picking was performed on MS1. ## Profile settings: method = bin ## step = 0.1 ## ## Memory usage: 0.776 MB Now you see there are 403 groups in the demo data, which meant 403 analytes are found across 4721 peaks. 9.4 Data correction Reasons of data correction might come from many aspects such as the unstable instrument and pollution on column. In xcms, the most important correction is retention time correction. Remember the original retention time might changed and use another object to save the new object: xset2 &lt;- retcor(xset, method = &quot;obiwarp&quot;) ## center sample: ko16 ## Processing: ko15 ko18 ko19 ko21 ko22 wt15 wt16 wt18 wt19 wt21 wt22 xset2 ## An &quot;xcmsSet&quot; object with 12 samples ## ## Time range: 2506.3-4162.2 seconds (41.8-69.4 minutes) ## Mass range: 200.1-599.3338 m/z ## Peaks: 4721 (about 393 per sample) ## Peak Groups: 0 ## Sample classes: KO, WT ## ## Peak picking was performed on MS1. ## Profile settings: method = bin ## step = 0.1 ## ## Memory usage: 0.713 MB # you need group the peaks again for this corrected data xset2 &lt;- group(xset2) ## 262 325 387 450 512 575 xset2 ## An &quot;xcmsSet&quot; object with 12 samples ## ## Time range: 2506.3-4162.2 seconds (41.8-69.4 minutes) ## Mass range: 200.1-599.3338 m/z ## Peaks: 4721 (about 393 per sample) ## Peak Groups: 404 ## Sample classes: KO, WT ## ## Peak picking was performed on MS1. ## Profile settings: method = bin ## step = 0.1 ## ## Memory usage: 0.776 MB You see one more peak groups after the correction. After the retention time correction, we also need to correct the peak groups by filling the missing peaks. Such function calls fillpeaks: xset3 &lt;- fillPeaks(xset2,BPPARAM=SnowParam()) ## /home/travis/R/Library/faahKO/cdf/KO/ko15.CDF ## /home/travis/R/Library/faahKO/cdf/KO/ko16.CDF ## /home/travis/R/Library/faahKO/cdf/KO/ko18.CDF ## /home/travis/R/Library/faahKO/cdf/KO/ko19.CDF ## /home/travis/R/Library/faahKO/cdf/KO/ko21.CDF ## /home/travis/R/Library/faahKO/cdf/KO/ko22.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt15.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt16.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt18.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt19.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt21.CDF ## /home/travis/R/Library/faahKO/cdf/WT/wt22.CDF xset3 ## An &quot;xcmsSet&quot; object with 12 samples ## ## Time range: 2502.9-4162.2 seconds (41.7-69.4 minutes) ## Mass range: 200.1-599.3338 m/z ## Peaks: 6054 (about 504 per sample) ## Peak Groups: 404 ## Sample classes: KO, WT ## ## Peak picking was performed on MS1. ## Profile settings: method = bin ## step = 0.1 ## ## Memory usage: 0.917 MB You see more peaks found. 9.5 Statistic analysis Right now we get peaks across samples, the next step is finding the differences between two groups. You will find the P values of t-test for pairwise comparison: reporttab &lt;- diffreport(xset3, &quot;WT&quot;, &quot;KO&quot;, &quot;example&quot;) reporttab[1:3,] ## name fold tstat pvalue mzmed mzmin mzmax ## 1 M300T3391 5.693594 14.44368 5.026336e-08 300.1898 300.1706 300.2000 ## 2 M301T3391 6.283030 15.52501 5.385022e-08 301.1879 301.1659 301.1949 ## 3 M298T3185 3.984984 11.88773 3.615841e-07 298.1508 298.1054 298.1592 ## rtmed rtmin rtmax npeaks KO WT ko15 ko16 ko18 ## 1 3390.699 3374.142 3398.743 12 6 6 4534353.6 4980914.5 5290739.1 ## 2 3391.126 3385.366 3394.937 7 6 1 962353.4 1047934.1 1109303.0 ## 3 3185.221 3182.083 3190.163 4 4 0 180780.8 204134.9 191015.9 ## ko19 ko21 ko22 wt15 wt16 wt18 wt19 ## 1 4564262.9 4733236.1 3931592.6 349660.89 491793.18 645526.70 634108.85 ## 2 946943.4 984787.2 806171.5 80639.28 118940.90 134531.39 102784.65 ## 3 190626.8 155276.9 220288.6 16448.42 41050.04 50082.55 76704.81 ## wt21 wt22 ## 1 1438254.45 1364627.84 ## 2 203982.76 291392.97 ## 3 53957.78 48363.33 Now you have got the ions that varies a lot between groups. Such ions are things we should take care of. In a ideal case, this is the endpoint of your study and the left work is making a report of your finding. However,we need q-values to control FDR. To get the q-values, you need input p-values and use the function from qvalue package. library(qvalue) # extract the p-value to caculate q-value qvalue &lt;- qvalue(p=reporttab$pvalue) # add qvalue to reporttab reporttab$qvalue &lt;- qvalue$qvalues # reporttab[1:3,] For further information about q-value, check here. After the FDR control, the following steps depend on your study. 9.6 Annotation I suggest CAMERA package to handle this task. You need to prepare an object of class xcmsSet, for example, xset3(remember to use fillpeaks to get the ions group). library(CAMERA) # Create an xsAnnotate object xsa &lt;- xsAnnotate(xset3) # Group after RT value of the xcms grouped peak xsaF &lt;- groupFWHM(xsa, perfwhm=0.6) ## Start grouping after retention time. ## Created 132 pseudospectra. # Verify grouping xsaC &lt;- groupCorr(xsaF) ## Start grouping after correlation. ## Generating EIC&#39;s .. ## ## Calculating peak correlations in 132 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## ## Calculating graph cross linking in 132 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## New number of ps-groups: 202 ## xsAnnotate has now 202 groups, instead of 132 # Annotate isotopes, could be done before groupCorr xsaFI &lt;- findIsotopes(xsaC) ## Generating peak matrix! ## Run isotope peak annotation ## % finished: 10 20 30 40 50 60 70 80 90 100 ## Found isotopes: 57 # Annotate adducts xsaFA &lt;- findAdducts(xsaFI, polarity=&quot;positive&quot;) ## Generating peak matrix for peak annotation! ## ## Calculating possible adducts in 202 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 # See the results getPeaklist(xsaFA)[1:3,] ## mz mzmin mzmax rt rtmin rtmax npeaks KO WT ## 1 200.1000 200.1000 200.1000 2924.027 2876.967 2939.450 9 4 5 ## 2 205.0000 205.0000 205.0000 2788.377 2782.719 2795.550 12 6 6 ## 3 205.9927 205.9786 206.0023 2789.144 2782.719 2793.925 12 6 6 ## ko15 ko16 ko18 ko19 ko21 ko22 wt15 ## 1 147887.5 451600.7 65290.38 52834.57 70042.53 162012.4 175177.1 ## 2 1778568.9 1567038.1 1482796.38 1039129.82 1223132.35 1072037.7 1950287.5 ## 3 237993.6 269714.0 201393.42 150107.31 176989.65 156797.0 276541.8 ## wt16 wt18 wt19 wt21 wt22 isotopes ## 1 82619.48 46255.03 69198.22 153273.5 98144.28 ## 2 1466780.60 1572679.16 1275312.76 1356014.3 1231442.16 [1][M]+ ## 3 222366.15 211717.71 186850.88 188285.9 172348.76 [1][M+1]+ ## adduct pcgroup ## 1 165 ## 2 [M+Na]+ 182.007 5 ## 3 5 # Get final peaktable and store on harddrive # write.csv(getPeaklist(xsaFA),file=&quot;data/result_CAMERA.csv&quot;) Any steps after the annotation could be operated solo and you may not need the isotopes or adducts. You could also use annotateDiffreport to show the results as diffreport in xcms. # make a diffreport with CAMERA result and extract the fold change higher than 3 dreport &lt;- annotateDiffreport(xset3, fc_th = 3) ## Start grouping after retention time. ## Created 132 pseudospectra. ## Generating peak matrix! ## Run isotope peak annotation ## % finished: 10 20 30 40 50 60 70 80 90 100 ## Found isotopes: 68 ## Start grouping after correlation. ## Generating EIC&#39;s .. ## ## Calculating peak correlations in 34 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## ## Calculating graph cross linking in 34 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## New number of ps-groups: 156 ## xsAnnotate has now 156 groups, instead of 132 ## Generating peak matrix for peak annotation! ## ## Calculating possible adducts in 58 Groups... ## % finished: 10 20 30 # extract the p-value to caculate q-value qvalue &lt;- qvalue(p=dreport$pvalue) # add qvalue to reporttab dreport$qvalue &lt;- qvalue$qvalues # See the results # dreport[1:3,] # save on harddrive # write.csv(dreport,file=&#39;data/diffreport.csv&#39;) 9.7 Omics analysis Since we have got the annotations, Omics analysis could be performed. In xcms, the default database is metlin. You could directly get the link to certain compounds when you generate the differences report. # make a diffreport with CAMERA result and extract the fold change higher than 3, add the metlin links dreport &lt;- annotateDiffreport(xset3, fc_th = 3, metlin = T) ## Start grouping after retention time. ## Created 132 pseudospectra. ## Generating peak matrix! ## Run isotope peak annotation ## % finished: 10 20 30 40 50 60 70 80 90 100 ## Found isotopes: 68 ## Start grouping after correlation. ## Generating EIC&#39;s .. ## ## Calculating peak correlations in 34 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## ## Calculating graph cross linking in 34 Groups... ## % finished: 10 20 30 40 50 60 70 80 90 100 ## New number of ps-groups: 156 ## xsAnnotate has now 156 groups, instead of 132 ## Generating peak matrix for peak annotation! ## ## Calculating possible adducts in 58 Groups... ## % finished: 10 20 30 # extract the p-value to caculate q-value qvalue &lt;- qvalue(p=dreport$pvalue) # add qvalue to reporttab dreport$qvalue &lt;- qvalue$qvalues # See the results dreport[1:3,] ## name fold tstat pvalue mzmed mzmin ## 300.2/3391 M300T3391 5.693594 -14.44368 5.026336e-08 300.1898 300.1706 ## 301.2/3391 M301T3391 6.283030 -15.52501 5.385022e-08 301.1879 301.1659 ## 298.2/3185 M298T3185 3.984984 -11.88773 3.615841e-07 298.1508 298.1054 ## mzmax rtmed rtmin rtmax npeaks KO WT ## 300.2/3391 300.2000 3390.699 3374.142 3398.743 12 6 6 ## 301.2/3391 301.1949 3391.126 3385.366 3394.937 7 6 1 ## 298.2/3185 298.1592 3185.221 3182.083 3190.163 4 4 0 ## metlin ## 300.2/3391 http://metlin.scripps.edu/metabo_list.php?mass_min=298.2&amp;mass_max=300.2 ## 301.2/3391 http://metlin.scripps.edu/metabo_list.php?mass_min=299.2&amp;mass_max=301.2 ## 298.2/3185 http://metlin.scripps.edu/metabo_list.php?mass_min=296.2&amp;mass_max=298.2 ## ko15 ko16 ko18 ## 300.2/3391 4534353.62273683 4980914.48421051 5290739.13866664 ## 301.2/3391 962353.429578945 1047934.14136842 1109303.04472222 ## 298.2/3185 180780.817277777 204134.864631578 191015.910842105 ## ko19 ko21 ko22 ## 300.2/3391 4564262.89684209 4733236.07999997 3931592.586 ## 301.2/3391 946943.392842103 984787.204999993 806171.472899999 ## 298.2/3185 190626.84952381 155276.902163857 220288.6218 ## wt15 wt16 wt18 ## 300.2/3391 349660.88536842 491793.181333331 645526.704947367 ## 301.2/3391 80639.2842881944 118940.899088542 134531.38671875 ## 298.2/3185 16448.4191894531 41050.0418122944 50082.5494449013 ## wt19 wt21 wt22 isotopes ## 300.2/3391 634108.848947367 1438254.44559999 1364627.844 [4][M]+ ## 301.2/3391 102784.647854275 203982.760512408 291392.971409092 [4][M+1]+ ## 298.2/3185 76704.8068359375 53957.7833573191 48363.333932977 ## adduct pcgroup qvalue ## 300.2/3391 17 1.087774e-05 ## 301.2/3391 17 1.087774e-05 ## 298.2/3185 103 4.869333e-05 # save on harddrive # write.csv(dreport,file=&#39;data/diffreport.csv&#39;) 9.8 MetaboAnalyst Actully, after you perform data correction, you have got the data matrix for statistic analysis. You might choose MetaboAnalyst online or offline to make furthor analysis, which supplied more statistical choices than xcms. The input data format for MetaboAnalyst should be rows for peaks and colomns for samples. You could also add groups infomation if possible. Use the following code to get the data for analysis. MAdata &lt;- groupval(xset3,method = &quot;medret&quot;, intensity = &quot;into&quot;) MAdata &lt;- rbind(group = as.character(phenoData(xset)$class),MAdata) # output the data for MetaboAnalyst # write.csv(MAdata, file = &quot;data/MAdata.csv&quot;) 9.9 Visulizing Peaks If you find some significant peaks, the best way to check them is data visulization. xcms supplies such functions. All you need are the retention time and ions’ range. eic &lt;- groups(xset3) index &lt;- which(eic[,&quot;rtmed&quot;] &gt; 2500 &amp; eic[,&quot;rtmed&quot;&lt;2600])[1] 9.10 Optimation of XCMS IPO package could be used to optimaze the parameters for XCMS. Try the following code. mzdatapath &lt;- system.file(&quot;cdf&quot;,package = &quot;faahKO&quot;) mzdatafiles &lt;- list.files(mzdatapath, recursive = TRUE, full.names=TRUE) library(IPO) peakpickingParameters &lt;- getDefaultXcmsSetStartingParams(&#39;matchedFilter&#39;) #setting levels for min_peakwidth to 10 and 20 (hence 15 is the center point) peakpickingParameters$min_peakwidth &lt;- c(10,20) peakpickingParameters$max_peakwidth &lt;- c(26,42) #setting only one value for ppm therefore this parameter is not optimized peakpickingParameters$ppm &lt;- 20 resultPeakpicking &lt;- optimizeXcmsSet(files = mzdatafiles[6:9], params = peakpickingParameters, nSlaves = 4, subdir = &#39;rsmDirectory&#39;) optimizedXcmsSetObject &lt;- resultPeakpicking$best_settings$xset retcorGroupParameters &lt;- getDefaultRetGroupStartingParams() retcorGroupParameters$profStep &lt;- 1 resultRetcorGroup &lt;- optimizeRetGroup(xset = optimizedXcmsSetObject, params = retcorGroupParameters, nSlaves = 4, subdir = &quot;rsmDirectory&quot;) writeRScript(resultPeakpicking$best_settings$parameters, resultRetcorGroup$best_settings, nSlaves=12) # https://github.com/rietho/IPO/blob/master/vignettes/IPO.Rmd 9.11 Summary This is the offline metaboliomics data process workflow. For each study, details would be different and F1 is always your best friend. Enjoy yourself in data mining! "],
["references.html", "References", " References "]
]
